\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, mathtools, graphicx}

\title{Automatic Differentiation of Stochastic Equations}
\author{}
\date{\today}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		This document provides an introduction to automatic differentiation (AD) techniques applied to stochastic differential equations (SDEs). We discuss theoretical foundations, computational methods, and applications. Exercises with solutions are provided to reinforce key concepts.
	\end{abstract}
	
	\section{Introduction}
	Stochastic differential equations (SDEs) play a crucial role in various fields, including finance, physics, and engineering. Differentiating functionals of SDEs is essential for optimization and sensitivity analysis. Automatic differentiation (AD) provides a powerful tool for computing derivatives with high precision and efficiency.
	
	This book covers:
	\begin{itemize}
		\item Fundamentals of stochastic calculus and SDEs,
		\item Principles of automatic differentiation,
		\item Application of AD to stochastic processes,
		\item Computational techniques and implementation.
	\end{itemize}
	
	\section{Background on Stochastic Calculus}
	A stochastic differential equation takes the form:
	\begin{equation}
		dX_t = f(X_t, t) dt + g(X_t, t) dW_t,
	\end{equation}
	where $W_t$ is a Wiener process, and $f$, $g$ are suitable functions. The It\^o lemma is fundamental for differentiating functionals of stochastic processes:
	\begin{equation}
		dF(X_t, t) = \frac{\partial F}{\partial t} dt + \frac{\partial F}{\partial X} dX_t + \frac{1}{2} \frac{\partial^2 F}{\partial X^2} g^2 dt.
	\end{equation}
	
	\section{Principles of Automatic Differentiation}
	Automatic differentiation decomposes a function into elementary operations and applies the chain rule systematically. AD methods can be categorized into:
	\begin{itemize}
		\item Forward mode AD: Efficient for functions with small input dimensions,
		\item Reverse mode AD: Efficient for functions with large input dimensions, common in machine learning and sensitivity analysis.
	\end{itemize}
	
	For a function $y = f(x)$, forward mode computes:
	\begin{equation}
		\dot{y} = f'(x) \dot{x},
	\end{equation}
	where $\dot{x}$ represents the derivative seed. Reverse mode propagates sensitivities backwards:
	\begin{equation}
		\bar{x} = \bar{y} f'(x).
	\end{equation}
	
	\section{Applying AD to Stochastic Equations}
	Differentiating functionals of SDEs requires combining AD with stochastic calculus. Given an expectation:
	\begin{equation}
		J(\theta) = \mathbb{E}[F(X_T; \theta)],
	\end{equation}
	where $X_T$ follows an SDE parameterized by $\theta$, we compute $\nabla_\theta J$ using:
	\begin{enumerate}
		\item Pathwise differentiation (if the expectation is smooth),
		\item Score function method (likelihood ratio method),
		\item Adjoint methods via reverse mode AD.
	\end{enumerate}
	
	\section{Exercises}
	\subsection{Exercise 1: Differentiation of Expectations}
	Given the functional:
	\begin{equation}
		J(\theta) = \mathbb{E}[e^{-\theta X_T}],
	\end{equation}
	where $X_T$ satisfies $dX_t = -X_t dt + dW_t$, compute $\frac{dJ}{d\theta}$ using pathwise differentiation.
	
	\subsection{Exercise 2: Reverse Mode AD for SDEs}
	Implement a reverse mode AD algorithm to compute the gradient of:
	\begin{equation}
		J(\theta) = \mathbb{E}[X_T^2]
	\end{equation}
	for the SDE $dX_t = \theta X_t dt + dW_t$.
	
	\section{Solutions}
	\subsection{Solution to Exercise 1}
	We first compute the expectation:
	\begin{equation}
		J(\theta) = \mathbb{E}[e^{-\theta X_T}].
	\end{equation}
	Taking the derivative under expectation:
	\begin{align*}
		\frac{dJ}{d\theta} &= \mathbb{E}\left[\frac{d}{d\theta} e^{-\theta X_T}\right] \\
		&= \mathbb{E}\left[-X_T e^{-\theta X_T}\right].
	\end{align*}
	Thus, the solution is:
	\begin{equation}
		\frac{dJ}{d\theta} = -\mathbb{E}[X_T e^{-\theta X_T}].
	\end{equation}
	
	\subsection{Solution to Exercise 2}
	We define the adjoint variable:
	\begin{equation}
		\bar{X}_T = \frac{dJ}{dX_T} = 2X_T.
	\end{equation}
	Using backpropagation through discretized dynamics, we compute:
	\begin{equation}
		\frac{dJ}{d\theta} = \mathbb{E} \left[ \int_0^T 2X_t \frac{\partial X_T}{\partial \theta} dt \right].
	\end{equation}
	Expanding $X_T$ explicitly:
	\begin{equation}
		X_T = X_0 e^{\theta T} + \int_0^T e^{\theta (T - s)} dW_s.
	\end{equation}
	Differentiating with respect to $\theta$:
	\begin{equation}
		\frac{\partial X_T}{\partial \theta} = T X_0 e^{\theta T} + \int_0^T (T - s) e^{\theta (T - s)} dW_s.
	\end{equation}
	Substituting into the integral:
	\begin{equation}
		\frac{dJ}{d\theta} = \mathbb{E} \left[ \int_0^T 2X_t \left(T X_0 e^{\theta T} + \int_0^T (T - s) e^{\theta (T - s)} dW_s \right) dt \right].
	\end{equation}
	This final expression provides an explicit form for computing the gradient using reverse mode AD.
	
	\section{Conclusion}
	We introduced AD techniques for SDEs, including forward and reverse mode methods. These techniques enable efficient sensitivity analysis in stochastic models.
	
	\bibliographystyle{plain}
	\bibliography{references}
	
\end{document}

