{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756bdd82-6c20-4e3d-999a-7bd7497bfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################\n",
      "# training start #\n",
      "##################\n",
      "\n",
      "Thu Oct 24 00:09:58 2024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from model_settings import ms\n",
    "os.chdir(os.path.abspath(str(Path())))\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "notebook_dir = str(Path().resolve())\n",
    "sys.path.append(os.path.join(notebook_dir,'historical_data','historical_generation'))\n",
    "train_start = time.time()\n",
    "train_start_datetime = datetime.fromtimestamp(train_start)\n",
    "train_start_tag = train_start_datetime.strftime('%c')\n",
    "print(\"\\n\"+\"#\"*18+\"\\n# training start #\\n\"+\n",
    "      \"#\"*18+\"\\n\"+f\"\\n{train_start_tag}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108afe68-dd3e-4537-8406-cb618ca7c4a2",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e3ac40-cbf2-43bc-8b3e-cdbc6f4b4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day_count': <QuantLib.QuantLib.Actual365Fixed; proxy of <Swig Object of type 'QuantLib::Actual365Fixed *' at 0x10c2c9aa0> >,\n",
       " 'calendar': <QuantLib.QuantLib.UnitedStates; proxy of <Swig Object of type 'QuantLib::UnitedStates *' at 0x11a74fc90> >,\n",
       " 'compounding': 2,\n",
       " 'frequency': 1,\n",
       " 'settings_names_dictionary': {2: 'continuous', 1: 'annual'},\n",
       " 'settings_string': '\\npricing settings:\\nActual/365 (Fixed) day counter\\nNew York stock exchange calendar\\ncompounding: continuous\\nfrequency: annual\\n',\n",
       " 'av_key': '9ZDGLN9SFCLWFN32',\n",
       " 'n_MCpaths': 100000,\n",
       " 'rng': 'pseudorandom',\n",
       " 'bloomberg_spx_calibrated': 'OneDrive - rsbrc/DATA/calibrated/bloomberg/SPX',\n",
       " 'calibrations_dir': 'OneDrive - rsbrc/DATA/calibrated',\n",
       " 'bloomberg_spx_barrier_dump': 'OneDrive - rsbrc/DATA/generated/bloomberg/SPX/barrier_options',\n",
       " 'cboe_spx_barrier_dump': 'OneDrive - rsbrc/DATA/generated/cboe/SPX/barrier_options',\n",
       " 'bloomberg_spx_asian_option_dump': 'OneDrive - rsbrc/DATA/generated/bloomberg/SPX/asian_options',\n",
       " 'cboe_spx_asian_option_dump': 'OneDrive - rsbrc/DATA/generated/cboe/SPX/asian_options'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c5e82f-7486-4c6b-aac9-237735ffe2df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(datadir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datadir,f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[0;32m----> 5\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs,ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m      7\u001b[0m dataset\n",
      "File \u001b[0;32m~/myvenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myvenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/myvenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myvenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myvenv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root = Path().resolve().parent.parent\n",
    "datadir = os.path.join(root,ms.cboe_spx_asian_option_dump)\n",
    "files = [f for f in os.listdir(datadir) if f.endswith('.csv')]\n",
    "files = [os.path.join(datadir,f) for f in files]\n",
    "dfs = [pd.read_csv(f).iloc[:,1:] for f in files]\n",
    "dataset = pd.concat(dfs,ignore_index=True) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e489495-1b34-4f16-b034-35c6e8b8acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import vanilla_pricer\n",
    "vanillas = vanilla_pricer()\n",
    "dataset['calculation_date'] = pd.to_datetime(dataset['calculation_date'],format='%Y-%m-%d')\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db510b1b-41a0-44f0-b879-c8edb00e9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[:,'vanilla'] = vanillas.df_heston_price(dataset)\n",
    "dataset.loc[:,'difference'] = dataset['vanilla'] -  dataset['asian_price']\n",
    "dataset.loc[:,'moneyness'] = ms.vmoneyness(dataset['spot_price'],dataset['strike_price'],dataset['w'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e52a7-7dbb-464d-a638-54108c2eb45b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6206662-c9d3-4b83-a0fb-c12717bee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import convsklearn\n",
    "categorical_features = ['averaging_type', 'w']\n",
    "numerical_features = [\n",
    "    'spot_price',\n",
    "    'strike_price',\n",
    "    'days_to_maturity',\n",
    "    'risk_free_rate',\n",
    "    'dividend_rate',\n",
    "    'kappa',\n",
    "    'theta',\n",
    "    'rho',\n",
    "    'eta',\n",
    "    'v0',\n",
    "    'fixing_frequency',\n",
    "    'n_fixings',\n",
    "    'past_fixings'\n",
    "]\n",
    "target_name = 'observed_price'\n",
    "trainer = convsklearn.convsklearn(categorical_features = categorical_features, numerical_features = numerical_features, target_name = target_name)\n",
    "for col in trainer.numerical_features:\n",
    "    dataset[col] = pd.to_numeric(dataset[col],errors='coerce')\n",
    "dataset['asian_price'] = pd.to_numeric(dataset['asian_price'],errors='coerce')\n",
    "dataset['observed_price'] = ms.noisyfier(dataset['asian_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc232ac-17b4-4341-9c0b-bf9ab5a58ad4",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "981af736-980b-4399-9177-c34e93b70e6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "unique_dates = dataset['calculation_date'].sort_values(\n",
    "    ascending=True).unique().tolist()\n",
    "filter_date = unique_dates[int(0.85*len(unique_dates))]\n",
    "train_data = dataset[\n",
    "    (\n",
    "      # (dataset['calculation_date']>=datetime(2007,1,1))\n",
    "      #  &\n",
    "      (dataset['calculation_date']<=filter_date)\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "test_data = dataset[\n",
    "    (\n",
    "        (dataset['calculation_date']>filter_date)\n",
    "        # &\n",
    "        # (dataset['calculation_date']<=datetime(2012,12,31))\n",
    "    )\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12152837-6cba-43ad-ac30-124ec0368a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['n_fixings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024dab2-ca6e-406f-abd9-bceb3cdacbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[dataset['n_fixings']==1]\n",
    "train_data = dataset[dataset['n_fixings']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80561f1c-a10a-405d-bc8a-b947b8cb31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae205b8c-f9f1-4850-9995-8830661b173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = int(round(100*test_data.shape[0]/train_data.shape[0],0))\n",
    "print(f\"train/test: {100-test_train_ratio}/{test_train_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85944d-4371-482f-b39e-9431270ca5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrs = trainer.get_train_test_arrays(\n",
    "    train_data, test_data,feature_set = trainer.feature_set, target_name=trainer.target_name)\n",
    "preprocessor = trainer.preprocess()\n",
    "train_X = arrs['train_X'] \n",
    "train_y = arrs['train_y']\n",
    "test_X = arrs['test_X']\n",
    "test_y = arrs['test_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60807a-e929-4919-bd91-12539c89689b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b25bd4-4e1d-4f21-8963-ab86f780d371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fit, runtime, specs = trainer.run_dnn(preprocessor,train_X,train_y)\n",
    "train_end = time.time()\n",
    "train_runtime = train_end-train_start\n",
    "print(f\"\\ncpu: {train_runtime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6405bd-60e1-43f3-90b5-21dc0b8eb241",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309a771-7200-49b7-9871-8fa9da1843d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401ccf-cc21-4867-9a9b-bf5b6a269418",
   "metadata": {},
   "outputs": [],
   "source": [
    "insample, outsample, errors = trainer.test_prediction_accuracy(\n",
    "        model_fit,\n",
    "        test_data,\n",
    "        train_data\n",
    "        )\n",
    "outofsample_RMSE = errors['outofsample_RMSE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d455a-0c63-4d2d-8ee6-0c26f2a11163",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6983915d-6693-4b77-aea0-8046f131ae54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "train_end_tag = str(datetime.fromtimestamp(\n",
    "    train_end).strftime(\"%Y_%m_%d %H-%M-%S\"))\n",
    "file_tag = str(train_end_tag + \" \" + specs[0] + \" \" + str(int(outofsample_RMSE)) + \"RMSE\")\n",
    "os.chdir(os.path.join(notebook_dir,'trained_models'))\n",
    "files_dir = os.path.join(\n",
    "    notebook_dir,'trained_models','trained_models',\n",
    "    file_tag)\n",
    "\n",
    "if Path(files_dir).exists():\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(files_dir)\n",
    "\n",
    "file_dir = os.path.join(files_dir,file_tag)\n",
    "\n",
    "S = np.sort(train_data['spot_price'].unique())\n",
    "K = np.sort(train_data['strike_price'].unique())\n",
    "T = np.sort(train_data['days_to_maturity'].unique())\n",
    "W = np.sort(train_data['w'].unique())\n",
    "n_calls = train_data[train_data['w']=='call'].shape[0]\n",
    "n_puts = train_data[train_data['w']=='put'].shape[0]\n",
    "insample.to_csv(f\"{file_dir} insample.csv\")\n",
    "outsample.to_csv(f\"{file_dir} outsample.csv\")\n",
    "joblib.dump(model_fit,str(f\"{file_dir}.pkl\"))\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "with open(f'{file_dir}.txt', 'w') as file:\n",
    "    file.write(train_start_tag)\n",
    "    file.write(f\"\\nspot(s):\\n{S}\")\n",
    "    file.write(f\"\\n\\nstrikes:\\n{K}\\n\")\n",
    "    file.write(f\"\\nmaturities:\\n{T}\\n\")\n",
    "    file.write(f\"\\ntypes:\\n{W}\\n\")\n",
    "    try:\n",
    "        file.write(f\"\\n{train_data['barrier_type_name'].unique()}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        for col in ['averaging_type','fixing_frequency','past_fixings','n_fixings']:\n",
    "            file.write(f\"\\n{col}:\")\n",
    "            file.write(f\"\\n{dataset[col].drop_duplicates().sort_values().values}\\n\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    file.write(f\"\\nnumber of calls, puts:\\n{n_calls},{n_puts}\\n\")\n",
    "    file.write(f\"\\ntotal prices:\\n{train_data.shape[0]}\\n\")\n",
    "    for spec in specs:\n",
    "        file.write(f\"{spec}\\n\")\n",
    "    file.write(\"#\"*17+\"\\n# training data #\\n\"+\"#\"*17+\n",
    "          f\"\\n{train_data.describe()}\\n\")\n",
    "    file.write(\"#\"*13+\"\\n# test data #\\n\"+\"#\"*13+\n",
    "          f\"\\n{test_data.describe()}\\n\")\n",
    "    file.write(f\"\\n{dataset.dtypes}\")\n",
    "    file.write(\n",
    "        f\"\\nin sample results:\"\n",
    "        f\"\\n     RMSE: {errors['insample_RMSE']}\"\n",
    "        f\"\\n     MAE: {errors['insample_MAE']}\\n\"\n",
    "        f\"\\nout of sample results:\"\n",
    "        f\"\\n     RMSE: {errors['outofsample_RMSE']}\"\n",
    "        f\"\\n     MAE: {errors['outofsample_MAE']}\\n\"\n",
    "        )\n",
    "    file.write(\"\\nfeatures:\\n\")\n",
    "    for feature in trainer.feature_set:\n",
    "        file.write(f\"     {feature}\\n\")\n",
    "    file.write(f\"\\ntarget: {trainer.target_name}\\n\")\n",
    "    file.write(f\"\\ncpu: {train_runtime}\\n\")\n",
    "    file.write(datetime.fromtimestamp(train_end).strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f4b68-d8cc-4756-b759-5309c84188fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
